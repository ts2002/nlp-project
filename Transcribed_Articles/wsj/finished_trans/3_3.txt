 Did you know you can listen to this show ad-free on Amazon Music included with your prime membership? To start listening, download the Amazon Music app for free and catch up on the latest episodes without the ads. Hey what's news listeners, it's Sunday March 3rd. I'm Luke Vargas. And I'm Amary for Tolley for the Wall Street Journal. This is what's news Sunday, the show where we tackle the big questions about the biggest stories in the news. We reach out to our colleagues across the newsroom to help explain what's happening in our world. This week, we're answering your questions about how social media is affecting kids. What platforms are doing to limit known harms for young people's mental health, and whether the government needs to intervene, and if so, how. You ready? Let's get right to it. When we asked you to share your concerns about social media's influence on kids, we heard back about a wide array of issues, from bullying and constant screen time to the formation of potentially addictive habits. And add to that, reporting on how social media can worsen teens' body image, make them targets of sexual harassment, and increase their exposure to content related to self-harm and racism, and it can feel overwhelming to parents to consider what ought to be done to make social media a safer place. Social media companies, including Meta and TikTok, have said they're committed to making sure their platforms are safe for kids. Meta told us it wants to help teens have safe, positive experiences on its apps. TikTok, X, and Apple did not respond to our requests for comment. As for government regulation, that is on the table. More on that later. But first, I'm joined by Julie Jargon, the journal's family and tech columnist, and reporter Jeff Horwitz, who covers Meta and led coverage of the award-winning Facebook files investigation. Jeff at a Senate hearing in January met a CEO Mark Zuckerberg, said the following. Mental health is a complex issue, and the existing body of scientific work has not shown a cause-a-link between using social media and young people having more mental health outcomes. Jeff, that comes after the US Surgeon General last year issued a public advisory warning about social media's risks for young people, citing this growing body of research on the detrimental effects for adolescents amid a national youth mental health crisis. So, to what extent are social media companies aware of those concerns? So, while I can't speak for everyone, there is unquestionably a large body of publicly available research outlining some of these issues, and then the stuff that we have obtained from inside Meta Facebook and Instagram research into things like notifications that, you know, Meta would sort of send. Basically, if they hadn't been checking the app enough per Meta's sensibility, those things were sort of geared toward increasing a fear of missing out and of kind of side-tracking kids from what they've been doing. And then there's also things, and the body image front as well, they did a lot of research into beauty filters, which the research found was bad for both the people posting them because they knew they didn't actually look like their online persona, and also bad for the people who saw them, because they definitely didn't look like the enhanced image of the people they were viewing. In terms of the techniques, Jeff, that social media companies use to drive engagement, you talked about systems built to maximize attention, to trigger FOMO, and I want to play a question that we received from one of our listeners, Mateo Guillemont. Here it is. Given that we've seen the research shows they're using addictive strategies to hook miners onto their platforms, you can consider these companies almost like gaming companies as a casino, for example, or online betting platform. Jeff, I should note, Mateo goes on to ask about regulation, but just on that claim about the use of addictive strategies to hook miners. Is that something that has been proven? The platforms themselves have been by their own acknowledgement, designing products to be stickier to involve people spending more time on them, and that is going to definitely involve some of the same interactions and dopamine rewards that slot machines provide, right? The analogy is not a bad one. It's actually part of a multi-state lawsuit claiming that meta designed its products to capitalize on how young users, brains, work, meta told us it doesn't design its products to be stickier and to encourage users to spend more time on them. Nevertheless, social media does have an impact, something you usually have reported on extensively. What has jumped out to you in the course of your reporting about how all of this is impacting families? Well, I think in terms of families, the major concern is that parents feel fairly helpless here. They feel like they're competing against an algorithm that is more efficient and effective than they are in shaping their children's online behavior. And they don't know how to get around that, short of not allowing their children to be on any of these platforms at all. So, once the kids are on social media, they're at a loss to figure out how to limit the time that they spend on these platforms and also restrict the type of content that they're seeing. It can be very confusing for parents to understand what an algorithm is showing their kids, and even though there are tools that the social media companies have developed to try to assist with this, it could be very cumbersome to set up. And of course, nothing is foolproof, and kids can find workarounds to anything parents do anyway. And on what Julie said there just to follow up, a lot of platforms make a big deal about the parental controls. It's true they exist, and they should exist. That said, the widely understood secret inside of most of the companies is that the idea that even the most helicoptering parent is going to be able to keep track of their child's digital footprint is fairly implausible. Julie, let me follow up on that because we heard from a listener, Nathan Beechy, who had this question about tools out there to help kids and really anyone control their experience on social media. Let's play that. I was wondering if there'd be a way for social media companies to use screen limiters or timers to enforce that individuals couldn't be on their platform for more than certain time each day. When you're reporting, you've referred to these types of tools when they're used to regulate children's time on social media as childproofing tools. Just remind us if you could what tools are out there and if they're considered safe. Well, the social media platforms themselves do have a number of tools, some of which can limit the amount of time. Kids spend on them, some are more in the lines of sort of content filtering where you can go in and pick certain hashtags that would be contained in videos that you might not want your child to see. They have a lot of tools that are not necessarily inherently bad, but like Jeff was saying, they're hard to use. They're hidden in the settings. They're not intuitive. And you're talking about several different social media apps that your kids might have across different devices. Then there are also device level settings that parents can use such as on Apple devices. And those have not worked very well either. Everett and about Apple's screen time settings, which allow parents to set time limits for their kids and do some other filtering. And there have been sinking problems across the parents devices and their kids devices that have made those settings come undone. So there's no certainty that parents can have that anything that they set up either sticks or that their kids can't find workarounds to. And Apple has said it will continue to make updates and improve that situation. Meta says that it's added 30 different tools and features to help parents and teens. But Julie, beyond these types of tools, what else are our social media companies doing in response to many of the concerns we've been talking about here? One of the recent moves that Meta has taken with Instagram is that they are going to automatically filter certain types of content from teen accounts. So certain body image related, like eating disorder related content, graphic violence, things like that. They're going to filter from getting to teen accounts. And again, teens can get around that if they feel like they're not seeing the stuff they used to see on their feeds. They can create a different account. They can lie about their age. And that gets into all of the kind of the big picture questions about how do you regulate this? For their part, social media companies like Meta, Jeff, have indicated they would like the government to take the lead on any regulations. Meta CEO Mark Zuckerberg told Senators at a hearing in January, he supports quote, setting industry standards on age, appropriate content. But Jeff, beyond what we've been discussing here, are there additional steps that social media companies could take if they wish to? Oh, vastly. There are steps that could be taken here. Look, this stuff wasn't designed for what parents would want. Or even what teens necessarily would want if you asked them. It was designed for the benefit of the companies. So a few things that definitely could be considered. First one is just what level of contact with the outside world you want to have. Do you want kids trying to go viral and being reached by basically anyone in the world? That is a product design choice. There's the question of what type of content gets recommended. And if you're doing it without much of an idea of what actually is being pushed. I mean, Julie mentioned the goal of not recommending awful things to kids. But the systems inside these companies are not good enough to reliably do that. There are a bunch of ways that you could make the product something that most parents would probably be more comfortable with. But it also would not be as good for usage of these products. And that's something that it's very hard to imagine the company is wanting to undertake voluntarily. Reporter Jeff Horowitz covers meta and social media platforms for us. And Julie Jargon is the Wall Street Journal's family and tech columnist. Jeff, Julie, thank you both so much. Thank you. Thanks for having me. Coming up, we'll shift this conversation to Washington to see how lawmakers are approaching potential regulations around kids and social media with the Brookings institutions Nicole Turner Lee. That's after a short break. Hey, what's news fans? Listen up. Did you know you can listen to episodes of this very show Add Free on Amazon Music included with your prime membership? That's right. All your favorite what's news episodes can be heard on Amazon Music Add Free. But that's not all. You get access to other fan favorite shows like The Daily, Up First and Fox News rundown Add Free as well. Amazon Music has all you need to stay up to date on all things newsworthy by offering the most Add Free top podcasts. So we know they definitely have something for you. And it's already included in your prime membership. To start listening, download the Amazon Music app for free or visit Amazon.com slash what's news. That's Amazon.com slash what's news. It's just that easy. Well, we've heard your concerns now about how social media is affecting children and some of what social media companies are or aren't doing to address those issues. But what about the role of regulators and lawmakers? Current federal legislation protects children's personal information lays out rules for what you can or cannot do online and sets out some expectations for how companies handle data. But that said, most of these laws were implemented before the proliferation of social media we see today. Nicole Turner Lee is the director of the Center for Technology Innovation at the nonprofit Brookings Institution where she steers research on tech policymaking. Nicole, we are going to talk about the potential for new legislation regulating social media. Many of these companies have said that they want the government to craft regulations. Is that something lawmakers could actually look to do? There's a general understanding today and quite obvious among some of the previous legislation that a threshold of a minor at 13 and under is basically one that most people buy into. The challenge is without age assurance a 13 year old could be online on a site that is designed for an 18 year old. There have been calls to sort of raise the age limit to 16. Like my daughter says it's like the driving limit of online platforms, right? But the challenge with the online space is that somebody has to verify that, right? There's going to be a parent that's going to be able to come on over and say, oh yeah, baby, let me push this button for you because you are at the right age. We're finding that that's not the case. There's some legislation that is suggesting we should have documentation. And then there's another interesting discussion that sort of has entered the stage here, which are young people who are going to the internet for protection and safety. Young people from the LGBTQ community who may not have told their parents that they're out but yet are going online to find a comfort group, trans kids. You know, these are like real issues which suggest to me this is not your grandmother's family household anymore, right? Not at all. And we actually heard from one listener Nicole Jordan who is wrestling with a very contemporary concern, wanting her kids to be tech savvy but also not exposed to content that would keep them glued to their screens. She told us that her soon to be 11 year old could soon get a paired down cell phone that doesn't include the internet or other apps. And there are a lot of companies out there making devices like this. Could there be something similar for social media? Well, I mean, here's the deal. When I was younger, my mother told me I couldn't listen to certain records and I still did. You know, we can get away with scaled down technology with our toddlers. I'm not sure if we can get away with that when we start dealing with our adolescents. We once did a podcast at Brookings where we bought young people on one of them, which was my daughter. And we asked them like, who's responsible for this? Do parents need to do more? Do we need to scale it down? And essentially they said we spend most of our high school education learning how to drive, but we don't learn about media. We don't have the digital literacy training. We won't give people the tools that they need to properly engage. And more importantly, we may actually intrude on children's rights. They're right to be able to be creative and ideate and other things that comes with these new technologies. Alright, so more education, more awareness needed here, but just going back to what you said earlier. If you can't regulate kids usage of the record playher or in this case, social media, how much can really be done here? We had one listener, Zoe Butler, Colin wondering what is actually politically feasible? I'm skeptical about bipartisan support for regulating social media for teens. I want to know the ways that these pieces of legislation play into the larger appetite for regulating big tech and social media algorithms. Nicole, be honest with us is bipartisan regulation on social media for children realistically within reach now. I mean, I think the ecosystem in which we have now when it comes to tech policy is still pretty disparate. We have various agencies asserting jurisdiction over parts of the ecosystem. You know, I think there is possibly more interest in children's privacy, believe it or not, than we're actually acknowledging. We've seen several bipartisan bills hit the floor. If we really want to hit children's privacy, we've got to hit privacy in general. Compared to the EU, the United States doesn't have a comprehensive data privacy standard. And that then allows for a wild west in terms of what can be collected on people in general. We do need to look at our outdated children's privacy laws, some of the things that are already pre-existing, and see if they actually function well in a 21st century economy. My point is, legislators can do something, the challenge that we have, and the Congress that we have, it's still under term in what they will do. Nicole Turnerley is the director of the Center for Technology Innovation at the Nonprofit Brookings Institution. Nicole, thank you so much for taking on these big questions with us. Thank you so much and I'm glad we got through it because these are really difficult questions. And that's it for what's new Sunday for March 3rd. Today's show was produced by Charlotte Gartenberg with supervising producer Sandra Kylhoff. We got help from Jonathan Sanders, Anthony Bansy, and Kate Bollivan, as well as Deputy Editor Scott Saloway, and Chris Sinsley. I'm Luke Vargas, and you'll hear from me again with a new show right here tomorrow morning. Until then, enjoy the rest of your Sunday. Thanks for listening. Did you know you can listen to this show ad-free on Amazon Music, included with your Prime Membership? To start listening, download the Amazon Music app for free and catch up on the latest episodes without the ads.